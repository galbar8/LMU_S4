{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from pathlib import Path\n",
   "id": "ea46783b93323e9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent.parent  # Go up from src/notebooks/ to project root\n",
    "data_root = str(project_root / \"src\" / \"datasets\" / \"esc50\" / \"data\")\n",
    "\n",
    "data_root"
   ],
   "id": "f326f8171933baef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# python\n",
    "import warnings\n",
    "\n",
    "PYTHONWARNINGS=\"ignore:In 2.9.*:UserWarning:torchaudio._backend.utils,ignore:'pin_memory'.*:UserWarning:torch.utils.data.dataloader\""
   ],
   "id": "97a13b102da6310e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys, pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src.utils.common import device_auto, count_params\n",
    "from src.models.classifier import SeqClassifier\n",
    "from src.models.lmu_block import LMUMemoryBlock\n",
    "\n",
    "# your project root (edit if needed)\n",
    "PROJECT_ROOT = pathlib.Path.cwd()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# dataset loader you already have\n",
    "from src.datasets.esc50.esc50_dataset import make_esc50_loaders\n",
    "from src.datasets.esc50.esc50_dataset2 import make_esc50_loaders as make_esc50_loaders2\n",
    "from tqdm import tqdm\n",
    "from src.utils.logging import Timer\n",
    "from src.utils.metrics import top1\n",
    "from src.utils.common import amp_autocast"
   ],
   "id": "27ea55464bc02438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# ---- Data ----\n",
    "DATA_ROOT            = data_root\n",
    "FOLD_VAL             = 1                   # 1..5\n",
    "FEATURE              = \"melspec\"           # or \"waveform\"\n",
    "SR                   = 16000\n",
    "N_MELS               = 128\n",
    "HOP_LENGTH           = 160 #320\n",
    "N_FFT                = 512 #1024\n",
    "TARGET_NUM_FRAMES    = 500 #250                 # ~5s @ 16k / hop=320\n",
    "AUGMENT              = True\n",
    "\n",
    "BATCH_SIZE           = 32\n",
    "NUM_WORKERS          = 4\n",
    "\n",
    "# ---- Training ----\n",
    "EPOCHS               = 150\n",
    "LR                   = 3e-4\n",
    "WEIGHT_DECAY         = 1e-3\n",
    "USE_AMP              = True\n",
    "SEED                 = 42\n",
    "\n",
    "# ---- Model (shared) ----\n",
    "N_CLASSES            = 50\n",
    "D_MODEL              = 256\n",
    "DEPTH                = 4\n",
    "DROPOUT              = 0.2\n",
    "MLP_RATIO            = 2.0\n",
    "\n",
    "# ---- LMU (external pkg) ----\n",
    "LMU_MEMORY_SIZE      = 256     # \"order\" Q\n",
    "LMU_THETA            = TARGET_NUM_FRAMES   # set to sequence length\n",
    "\n",
    "# ---- Logging / ckpts ----\n",
    "RUN_NAME             = f\"esc50_lmu_d{D_MODEL}x{DEPTH}_Q{LMU_MEMORY_SIZE}_fold{FOLD_VAL}\"\n",
    "SAVE_DIR             = PROJECT_ROOT / \"runs\" / RUN_NAME\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Save dir:\", SAVE_DIR)\n"
   ],
   "id": "2aec3aefe34036a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils.common import set_seed\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "train_loader, val_loader, cmvn_stats = make_esc50_loaders2(\n",
    "    data_root=DATA_ROOT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    fold_val=FOLD_VAL,\n",
    "    sample_rate=SR,\n",
    "    n_mels=N_MELS,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    normalize=\"global_cmvn\",\n",
    "    n_fft=N_FFT,\n",
    "    target_num_frames=TARGET_NUM_FRAMES,\n",
    "    augment=AUGMENT,\n",
    "    download=False,\n",
    "    wav_time_shift_pct=0.1,\n",
    "    wav_gain_db=3.0\n",
    ")\n",
    "\n",
    "xb, yb, _ = next(iter(train_loader))\n",
    "xb.shape, yb.shape  # expect: (B, T, D), (B,)\n"
   ],
   "id": "93f0e62323178f6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = device_auto()\n",
    "amp = bool(USE_AMP := (USE_AMP and device.type==\"cuda\"))\n",
    "writer = SummaryWriter(SAVE_DIR.as_posix())\n",
    "\n",
    "def build_model():\n",
    "    d_in = N_MELS if FEATURE==\"melspec\" else 1\n",
    "    def block_factory(d_model: int):\n",
    "        return LMUMemoryBlock(d_model, memory_size=LMU_MEMORY_SIZE, theta=LMU_THETA,\n",
    "                              dropout=DROPOUT, mlp_ratio=MLP_RATIO)\n",
    "    m = SeqClassifier(d_in, N_CLASSES, D_MODEL, DEPTH, block_factory)\n",
    "    return m\n",
    "\n",
    "model = build_model().to(device)\n",
    "print(f\"Params: {count_params(model):,}\")\n",
    "\n",
    "# opt = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, betas=(0.9,0.95))\n",
    "# sch = CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "#\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.reset_peak_memory_stats()\n",
    "#\n",
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# best_acc = 0.0\n",
    "# best_path = SAVE_DIR/\"best.pt\"\n",
    "\n",
    "# ==== Optimizer: WD=0 לפרמטרי bias/Norm ====\n",
    "decay, no_decay = [], []\n",
    "for n, p in model.named_parameters():\n",
    "    if p.ndim == 1 or n.endswith(\".bias\") or \"norm\" in n.lower():\n",
    "        no_decay.append(p)\n",
    "    else:\n",
    "        decay.append(p)\n",
    "optim_groups = [\n",
    "    {\"params\": decay, \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "]\n",
    "opt = AdamW(optim_groups, lr=LR, betas=(0.9, 0.95))\n",
    "\n",
    "# ==== Optimizer: WD=0 לפרמטרי bias/Norm ====\n",
    "decay, no_decay = [], []\n",
    "for n, p in model.named_parameters():\n",
    "    if p.ndim == 1 or n.endswith(\".bias\") or \"norm\" in n.lower():\n",
    "        no_decay.append(p)\n",
    "    else:\n",
    "        decay.append(p)\n",
    "optim_groups = [\n",
    "    {\"params\": decay, \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "]\n",
    "opt = AdamW(optim_groups, lr=LR, betas=(0.9, 0.95))\n",
    "\n",
    "# ==== Scheduler: warmup → cosine ====\n",
    "class WarmupThenCosine:\n",
    "    def __init__(self, optimizer, warmup_epochs, total_epochs):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.cur_epoch = 0\n",
    "        self.base_lrs = [g[\"lr\"] for g in optimizer.param_groups]\n",
    "        self.cosine = None\n",
    "\n",
    "    def _step_epoch(self):\n",
    "        self.cur_epoch += 1\n",
    "        if self.cur_epoch <= self.warmup_epochs:\n",
    "            scale = self.cur_epoch / max(1, self.warmup_epochs)\n",
    "            for pg, base_lr in zip(self.optimizer.param_groups, self.base_lrs):\n",
    "                pg[\"lr\"] = base_lr * scale\n",
    "        else:\n",
    "            if self.cosine is None:\n",
    "                self.cosine = CosineAnnealingLR(self.optimizer, T_max=self.total_epochs - self.warmup_epochs)\n",
    "            self.cosine.step()\n",
    "\n",
    "    def step(self):\n",
    "        self._step_epoch()\n",
    "\n",
    "sch_wrap = WarmupThenCosine(opt, warmup_epochs=3, total_epochs=EPOCHS)\n",
    "\n",
    "# ==== Loss ====\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# ==== AMP scaler (CUDA בלבד) ====\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "best_acc, best_path = 0.0, SAVE_DIR / \"best.pt\"\n"
   ],
   "id": "95ec2567f5dd27e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CLIP_NORM = 1.0\n",
    "\n",
    "def train_one_epoch(ep):\n",
    "    model.train()\n",
    "    total_loss=total_acc=0.0; n=0\n",
    "    with Timer() as t:\n",
    "        for xb,yb, _ in tqdm(train_loader, leave=False):\n",
    "            xb=xb.to(device,non_blocking=True); yb=yb.to(device,non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with amp_autocast(amp):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "            if amp and device.type == \"cuda\":\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "                scaler.step(opt); scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "                opt.step()\n",
    "\n",
    "            bs = xb.size(0)\n",
    "            total_loss += loss.item()*bs\n",
    "            total_acc  += top1(logits.detach(), yb)*bs\n",
    "            n += bs\n",
    "    return {\"loss\": total_loss/n, \"acc\": total_acc/n, \"time_s\": t.dt}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(ep):\n",
    "    model.eval()\n",
    "    total_loss=total_acc=0.0\n",
    "    n=0\n",
    "    with Timer() as t:\n",
    "        for xb,yb, _ in tqdm(val_loader, leave=False):\n",
    "            xb=xb.to(device,non_blocking=True); yb=yb.to(device,non_blocking=True)\n",
    "            with amp_autocast(amp):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            bs = xb.size(0)\n",
    "            total_loss += loss.item()*bs\n",
    "            total_acc  += top1(logits, yb)*bs\n",
    "            n += bs\n",
    "    return {\"loss\": total_loss/n, \"acc\": total_acc/n, \"time_s\": t.dt}\n"
   ],
   "id": "e75bb85d1a04aff2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# def train_one_epoch(ep):\n",
    "#     model.train_utils()\n",
    "#     total_loss=total_acc=0.0; n=0\n",
    "#     with Timer() as t:\n",
    "#         for xb,yb in tqdm(train_loader, leave=False):\n",
    "#             xb=xb.to(device,non_blocking=True); yb=yb.to(device,non_blocking=True)\n",
    "#             opt.zero_grad(set_to_none=True)\n",
    "#             with amp_autocast(amp):\n",
    "#                 logits = model(xb)\n",
    "#                 loss = criterion(logits, yb)\n",
    "#             if amp:\n",
    "#                 scaler.scale(loss).backward()\n",
    "#                 scaler.step(opt); scaler.update()\n",
    "#             else:\n",
    "#                 loss.backward(); opt.step()\n",
    "#             bs = xb.size(0)\n",
    "#             total_loss += loss.item()*bs\n",
    "#             total_acc  += top1(logits.detach(), yb)*bs\n",
    "#             n += bs\n",
    "#     return {\"loss\": total_loss/n, \"acc\": total_acc/n, \"time_s\": t.dt}\n",
    "#\n",
    "# @torch.no_grad()\n",
    "# def evaluate(ep):\n",
    "#     model.eval()\n",
    "#     total_loss=total_acc=0.0; n=0\n",
    "#     with Timer() as t:\n",
    "#         for xb,yb in tqdm(val_loader, leave=False):\n",
    "#             xb=xb.to(device,non_blocking=True); yb=yb.to(device,non_blocking=True)\n",
    "#             with amp_autocast(amp):\n",
    "#                 logits = model(xb)\n",
    "#                 loss = criterion(logits, yb)\n",
    "#             bs = xb.size(0)\n",
    "#             total_loss += loss.item()*bs\n",
    "#             total_acc  += top1(logits, yb)*bs\n",
    "#             n += bs\n",
    "#     return {\"loss\": total_loss/n, \"acc\": total_acc/n, \"time_s\": t.dt}"
   ],
   "id": "91e55945ae02697d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils.logging import gpu_mem_mb_peak\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_one_epoch(ep)\n",
    "    va = evaluate(ep)\n",
    "    sch_wrap.step()\n",
    "\n",
    "    # logs\n",
    "    writer.add_scalar(\"train_utils/loss\", tr[\"loss\"], ep)\n",
    "    writer.add_scalar(\"train_utils/acc\",  tr[\"acc\"],  ep)\n",
    "    writer.add_scalar(\"train_utils/time_s\", tr[\"time_s\"], ep)\n",
    "\n",
    "    writer.add_scalar(\"val/loss\", va[\"loss\"], ep)\n",
    "    writer.add_scalar(\"val/acc\",  va[\"acc\"],  ep)\n",
    "    writer.add_scalar(\"val/time_s\", va[\"time_s\"], ep)\n",
    "\n",
    "    writer.add_scalar(\"sys/gpu_mem_mb_peak\", gpu_mem_mb_peak(), ep)\n",
    "\n",
    "    # checkpoints\n",
    "    ckpt = {\"model\": model.state_dict(), \"epoch\": ep, \"val\": va,\n",
    "            \"config\": {\n",
    "              \"feature\": FEATURE, \"sr\": SR, \"n_mels\": N_MELS,\n",
    "              \"d_model\": D_MODEL, \"depth\": DEPTH, \"lmu_memory_size\": LMU_MEMORY_SIZE,\n",
    "            }}\n",
    "    # torch.save(ckpt, SAVE_DIR/f\"epoch_{ep:03d}.pt\")\n",
    "    # if va[\"acc\"] > best_acc:\n",
    "    #     best_acc = va[\"acc\"]\n",
    "    #     torch.save(ckpt, best_path)\n",
    "    #     print(f\"✅ new best acc {best_acc:.4f} @ epoch {ep}\")\n",
    "\n",
    "    print(f\"Epoch {ep}/{EPOCHS} | \"\n",
    "          f\"train_utils loss {tr['loss']:.4f} acc {tr['acc']:.4f} \"\n",
    "          f\"| val loss {va['loss']:.4f} acc {va['acc']:.4f} \"\n",
    "          f\"| t_train {tr['time_s']:.2f}s t_val {va['time_s']:.2f}s\")\n",
    "\n",
    "writer.close()\n",
    "print(\"Best ckpt:\", best_path)\n"
   ],
   "id": "ca52d70788199317",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
