{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "PYTHONWARNINGS=\"ignore:In 2.9.*:UserWarning:torchaudio._backend.utils,ignore:'pin_memory'.*:UserWarning:torch.utils.data.dataloader\"\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent.parent.parent\n",
    "data_root = str(project_root / \"src\" / \"datasets\" / \"esc50\" / \"data\")\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Data root: {data_root}\")"
   ],
   "id": "bdc55771e40b328f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import os, torch, torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.datasets.esc50.esc50_dataset2 import make_esc50_loaders\n",
    "from src.models.v2.build_model import BlockConfig, build_model\n",
    "from src.utils.common import set_seed, device_auto, amp_autocast, count_params\n",
    "from src.utils.logging import TB\n",
    "\n",
    "from src.train_utils.ema import EMA\n",
    "from src.train_utils.early_stopping import EarlyStopping\n",
    "from src.train_utils.loops import train_one_epoch, evaluate_one_epoch\n",
    "\n",
    "# Optional eval helpers\n",
    "from src.eval.infer import predict_loader\n",
    "from src.eval.metrics import confusion_matrix\n",
    "from src.eval.report import print_basic_report, plot_confusion, print_per_class\n",
    "\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "# --- experiment config ---\n",
    "args = SimpleNamespace(\n",
    "    # data\n",
    "    data_root=data_root,    # <-- set path with audio/ + meta/esc50.csv\n",
    "    batch=32,\n",
    "    workers=4,\n",
    "    fold_val=1,\n",
    "    feature=\"melspec\",\n",
    "    sample_rate=16000,\n",
    "    n_mels=128,\n",
    "    hop_length=160,                 # 10 ms hop → ~500 frames\n",
    "    n_fft=512,\n",
    "    target_num_frames=500,\n",
    "    augment=True,\n",
    "\n",
    "    # training\n",
    "    epochs=100,\n",
    "    lr=3e-4,\n",
    "    wd=1e-3,\n",
    "    amp=True,                       # works on CUDA and Apple MPS (via your amp_autocast)\n",
    "    seed=42,\n",
    "    save_dir=\"./runs/esc50_v2_nb\",\n",
    "    ema=True,\n",
    "    ema_decay=0.999,\n",
    "    label_smoothing=0.1,\n",
    "    warmup_epochs=5,\n",
    "    patience=20,\n",
    "    min_delta=0.0,\n",
    "\n",
    "    # model scaffold\n",
    "    core=\"lmu\",\n",
    "    d_model=256,\n",
    "    depth=6,\n",
    "    dropout=0.2,\n",
    "    mlp_ratio=2.0,\n",
    "    droppath_final=0.1,\n",
    "    layerscale_init=1e-2,\n",
    "    residual_gain=1.0,\n",
    "    pool=\"mean\",\n",
    ")"
   ],
   "id": "9f2a54d96d742dfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "set_seed(args.seed)\n",
    "device = device_auto()\n",
    "amp = bool(args.amp and device.type in {\"cuda\",\"mps\"})\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "loaders = make_esc50_loaders(\n",
    "    data_root=args.data_root, batch_size=args.batch, num_workers=args.workers,\n",
    "    fold_val=args.fold_val, feature=args.feature, sample_rate=args.sample_rate,\n",
    "    n_mels=args.n_mels, hop_length=args.hop_length, n_fft=args.n_fft,\n",
    "    target_num_frames=args.target_num_frames, augment=args.augment, download=False\n",
    ")\n",
    "\n",
    "if isinstance(loaders, (list, tuple)) and len(loaders) == 3:\n",
    "    train_loader, val_loader, _ = loaders\n",
    "else:\n",
    "    train_loader, val_loader = loaders\n",
    "\n",
    "print(f\"train size: {len(train_loader.dataset)}, validate size: {len(val_loader.dataset)}, device={device}\")"
   ],
   "id": "878ea20601a16e9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_classes = 50\n",
    "d_in = args.n_mels if args.feature == \"melspec\" else 1\n",
    "\n",
    "block_cfg = BlockConfig(\n",
    "    kind=args.core,\n",
    "    memory_size=256,\n",
    "    theta=args.target_num_frames,\n",
    "    dropout=args.dropout,\n",
    "    mlp_ratio=args.mlp_ratio,\n",
    "    droppath_final=args.droppath_final,\n",
    "    layerscale_init=args.layerscale_init,\n",
    "    residual_gain=args.residual_gain,\n",
    "    pool=args.pool,\n",
    ")\n",
    "\n",
    "model = build_model(\n",
    "    d_in=d_in, n_classes=n_classes, d_model=args.d_model, depth=args.depth, block_cfg=block_cfg\n",
    ").to(device)\n",
    "\n",
    "print(f\"device={device}\")\n",
    "print(f\"Params: {count_params(model):,}\")\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd, betas=(0.9, 0.95))\n",
    "\n",
    "warmup_epochs = max(0, int(args.warmup_epochs))\n",
    "if warmup_epochs > 0:\n",
    "    sch = SequentialLR(\n",
    "        opt,\n",
    "        schedulers=[\n",
    "            LinearLR(opt, start_factor=1e-3, end_factor=1.0, total_iters=warmup_epochs),\n",
    "            CosineAnnealingLR(opt, T_max=args.epochs - warmup_epochs),\n",
    "        ],\n",
    "        milestones=[warmup_epochs],\n",
    "    )\n",
    "else:\n",
    "    sch = CosineAnnealingLR(opt, T_max=args.epochs)\n",
    "\n",
    "\n",
    "if amp and device.type == \"cuda\":\n",
    "    try:\n",
    "        scaler = torch.amp.GradScaler(device=\"cuda\", enabled=True)\n",
    "    except AttributeError:  # older PyTorch\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "ema = EMA(model, decay=args.ema_decay) if args.ema else None\n",
    "tb = TB(args.save_dir)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing) if args.label_smoothing > 0 else nn.CrossEntropyLoss()\n"
   ],
   "id": "4378fa12ecbc3417",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_acc = 0.0\n",
    "best_path = os.path.join(args.save_dir, \"best.pt\")\n",
    "primed_scheduler = False\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "stopper = EarlyStopping(patience=args.patience, min_delta=args.min_delta)\n",
    "\n",
    "for ep in range(args.epochs + 1):\n",
    "    tr = train_one_epoch(model, train_loader, opt, scaler, device, amp, criterion, ema)\n",
    "    va = evaluate_one_epoch(model, val_loader, device, amp, criterion, ema)\n",
    "\n",
    "    # Scheduler AFTER at least one optimizer step\n",
    "    if tr.get(\"stepped\", False):\n",
    "        sch.step(); primed_scheduler = True\n",
    "    elif not primed_scheduler:\n",
    "        opt.step(); sch.step(); primed_scheduler = True\n",
    "\n",
    "    history[\"train_loss\"].append(tr[\"loss\"]); history[\"train_acc\"].append(tr[\"acc\"])\n",
    "    history[\"val_loss\"].append(va[\"loss\"]);   history[\"val_acc\"].append(va[\"acc\"])\n",
    "\n",
    "    tb.scalars(tr, ep, \"train/\"); tb.scalars(va, ep, \"val/\")\n",
    "    tb.scalars({\"lr\": tr[\"lr\"]}, ep, \"train/\")\n",
    "\n",
    "    print(f\"Epoch {ep:03d}/{args.epochs} | \"\n",
    "          f\"train {tr['loss']:.4f}/{tr['acc']:.4f} | \"\n",
    "          f\"val {va['loss']:.4f}/{va['acc']:.4f} | \"\n",
    "          f\"t {tr['time_s']:.1f}s/{va['time_s']:.1f}s | \"\n",
    "          f\"lr {tr['lr']:.2e}\")\n",
    "\n",
    "    ckpt = {\"model\": model.state_dict(), \"epoch\": ep, \"val\": va, \"args\": vars(args)}\n",
    "    # torch.save(ckpt, os.path.join(args.save_dir, f\"epoch_{ep:03d}.pt\"))\n",
    "    if va[\"acc\"] > best_acc:\n",
    "        best_acc = va[\"acc\"]\n",
    "        torch.save(ckpt, best_path)\n",
    "        print(f\"✅ new best acc {best_acc:.4f}\")\n",
    "\n",
    "    if stopper.step(va[\"acc\"]):\n",
    "        print(f\"⏹ Early stopping (patience={args.patience}, best_acc={stopper.best:.4f}).\")\n",
    "        break\n",
    "\n",
    "tb.close()\n",
    "best_acc"
   ],
   "id": "723df2e403c01eaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load best if desired\n",
    "best_ckpt = torch.load(os.path.join(args.save_dir, \"best.pt\"), map_location=\"cpu\")\n",
    "model.load_state_dict(best_ckpt[\"model\"])\n",
    "model.to(device)\n",
    "\n",
    "# Get logits/labels on validation set\n",
    "logits, labels = predict_loader(model, val_loader, device, amp_autocast, amp)\n",
    "\n",
    "# Basic report\n",
    "print_basic_report(logits, labels)\n",
    "\n",
    "# Confusion matrix + per-class stats\n",
    "cm = confusion_matrix(logits, labels, num_classes=50)\n",
    "plot_confusion(cm, class_names=None, normalize=True, figsize=(9,9))\n",
    "print_per_class(cm, class_names=None, top_k=10)\n"
   ],
   "id": "c6d17ab999600f6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.legend(); plt.title(\"Accuracy\"); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.title(\"Loss\"); plt.show()"
   ],
   "id": "55762c1162ceb6f7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
