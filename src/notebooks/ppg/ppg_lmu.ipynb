{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PPG Heart Rate Estimation with LMU\n",
    "\n",
    "Train LMU model on PPG dataset for continuous heart rate estimation.\n",
    "\n",
    "This notebook follows a standard regression workflow:\n",
    "- **Task**: Predict heart rate (bpm) from PPG windows\n",
    "- **Model**: LMU-based sequence model\n",
    "- **Training**: MSE loss with MAE tracking\n",
    "- **Evaluation**: Test set performance (MAE, RMSE)\n"
   ],
   "id": "f5a81336f347e49a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T21:17:42.136812Z",
     "start_time": "2025-11-18T21:17:42.128974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CRITICAL: Force reload all custom modules to ensure fixes are applied\n",
    "import sys\n",
    "\n",
    "modules_to_reload = [k for k in list(sys.modules.keys()) if k.startswith('src.')]\n",
    "for mod in modules_to_reload:\n",
    "    del sys.modules[mod]\n",
    "\n",
    "if modules_to_reload:\n",
    "    print(f\"üîÑ Cleared {len(modules_to_reload)} cached modules\")\n"
   ],
   "id": "e8308c8759c174a8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T21:17:47.716183Z",
     "start_time": "2025-11-18T21:17:46.492422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple, Dict, Any\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.types.task_protocol import TaskProtocol\n",
    "from src.datasets.ppg.ppg_config import PPGDaliaConfig\n",
    "from src.datasets.ppg.ppg_dataloader import make_ppgdalia_loaders\n",
    "from src.models.v2.build_model import BlockConfig\n"
   ],
   "id": "d725f73f1334b06c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled.\n",
      "Falling back on slow Cauchy and Vandermonde kernel. Install at least one of pykeops or the CUDA extension for better speed and memory efficiency.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Verify Shape Fix\n",
    "\n",
    "This cell verifies that the data loader and model output compatible shapes."
   ],
   "id": "f1c007cd584602ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T21:17:55.315823Z",
     "start_time": "2025-11-18T21:17:53.647794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SHAPE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Quick test to verify shapes\n",
    "from src.datasets.ppg.ppg_dataset import PPGDaliaDataset\n",
    "from src.datasets.ppg.ppg_dataloader import ppg_collate\n",
    "\n",
    "# Create a tiny test dataset\n",
    "test_cfg = PPGDaliaConfig(\n",
    "    root=str(Path.cwd().parent.parent.parent / \"src\" / \"datasets\" / \"ppg\" / \"data\"),\n",
    "    subjects_train=(\"S1\",),\n",
    "    fs_in=64.0, fs=100.0, win_sec=8, stride_sec=2,\n",
    "    do_bandpass=True, low_hz=0.5, high_hz=8.0, split='train'\n",
    ")\n",
    "test_ds = PPGDaliaDataset(test_cfg)\n",
    "\n",
    "# Get a batch\n",
    "batch = [test_ds[i] for i in range(min(4, len(test_ds)))]\n",
    "x_batch, y_batch, _ = ppg_collate(batch)\n",
    "\n",
    "print(f\"‚úÖ Batch x shape: {tuple(x_batch.shape)} (expected: [B, T, 1])\")\n",
    "print(f\"‚úÖ Batch y shape: {tuple(y_batch.shape)} (expected: [B])\")\n",
    "\n",
    "if y_batch.ndim == 1:\n",
    "    print(\"‚úÖ Target shape is correct: [B] for regression\")\n",
    "elif y_batch.shape[-1] == 1 and y_batch.ndim == 2:\n",
    "    print(\"‚ùå Target shape is [B, 1] but should be [B]\")\n",
    "    print(\"   ‚Üí Restart kernel and run again\")\n",
    "else:\n",
    "    print(f\"‚ùå Unexpected target shape: {tuple(y_batch.shape)}\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ],
   "id": "cffb3f830f47ea4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SHAPE VERIFICATION\n",
      "============================================================\n",
      "‚úÖ Batch x shape: (4, 800, 1, 1) (expected: [B, T, 1])\n",
      "‚úÖ Batch y shape: (4,) (expected: [B])\n",
      "‚úÖ Target shape is correct: [B] for regression\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task Definition",
   "id": "85d18c7d3da8290"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T21:18:04.782056Z",
     "start_time": "2025-11-18T21:18:04.775712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PPGTask(TaskProtocol):\n",
    "    \"\"\"PPG-based heart rate estimation (regression).\"\"\"\n",
    "    problem_type: str = \"regression\"\n",
    "\n",
    "    def make_loaders(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 4,\n",
    "        **kwargs\n",
    "    ) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"Create data loaders using PPGDaliaConfig.\"\"\"\n",
    "        # Separate dataloader kwargs from config kwargs\n",
    "        pin_memory = kwargs.pop(\"pin_memory\", False)\n",
    "        persistent_workers = kwargs.pop(\"persistent_workers\", False)\n",
    "\n",
    "        # Remaining kwargs go to PPGDaliaConfig\n",
    "        cfg = PPGDaliaConfig(root=data_root, **kwargs)\n",
    "\n",
    "        return make_ppgdalia_loaders(\n",
    "            cfg,\n",
    "            batch=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "        )\n",
    "\n",
    "    def infer_input_dim(self, args: Dict[str, Any]) -> int:\n",
    "        \"\"\"Single PPG channel.\"\"\"\n",
    "        return 1\n",
    "\n",
    "    def infer_num_classes(self, args: Dict[str, Any]) -> int:\n",
    "        \"\"\"Single HR output.\"\"\"\n",
    "        return 1\n",
    "\n",
    "    def infer_theta(self, args: Dict[str, Any]) -> int:\n",
    "        \"\"\"Sequence length = window size in samples.\"\"\"\n",
    "        win_sec = args.get(\"win_sec\", 8)\n",
    "        fs = args.get(\"fs\", 100.0)\n",
    "        return int(win_sec * fs)\n"
   ],
   "id": "79dc067079f729af",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Block Configuration Helper",
   "id": "f773159038c0295b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T21:18:08.382405Z",
     "start_time": "2025-11-18T21:18:08.377218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_block_cfg_ctor(dropout, mlp_ratio, droppath_final, layerscale_init, residual_gain, pool, memory_size=256):\n",
    "    \"\"\"Create LMU block config constructor (agnostic pattern for later S4 comparison).\"\"\"\n",
    "    def block_cfg_ctor(theta: int):\n",
    "        return BlockConfig(\n",
    "            kind=\"lmu\",\n",
    "            memory_size=memory_size,\n",
    "            theta=theta,\n",
    "            dropout=dropout,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            droppath_final=droppath_final,\n",
    "            layerscale_init=layerscale_init,\n",
    "            residual_gain=residual_gain,\n",
    "            pool=pool\n",
    "        )\n",
    "    return block_cfg_ctor\n"
   ],
   "id": "b10b9b0f017d4d39",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "\n",
    "**IMPORTANT**: Update `TRAIN_SUBJECTS`, `VAL_SUBJECT`, and `TEST_SUBJECT` to match your actual dataset subject IDs.\n",
    "\n",
    "Place your PPG data in: `src/datasets/ppg/data/`\n",
    "\n",
    "Expected structure:\n",
    "```\n",
    "data/\n",
    "  S1/\n",
    "    *.csv\n",
    "  S2/\n",
    "    *.csv\n",
    "  ...\n",
    "```\n",
    "\n",
    "Each CSV should contain columns: `ppg`, `hr`\n"
   ],
   "id": "ec60c0a9bc3b3c66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T21:18:11.958045Z",
     "start_time": "2025-11-18T21:18:11.927476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent.parent.parent  # from src/notebooks/ppg to project root\n",
    "data_root = str(project_root / \"src\" / \"datasets\" / \"ppg\" / \"data\")\n",
    "\n",
    "# Example subject split - ADJUST TO YOUR DATASET\n",
    "TRAIN_SUBJECTS = (\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\")\n",
    "VAL_SUBJECT = \"S9\"\n",
    "TEST_SUBJECT = \"S10\"\n",
    "\n",
    "args: Dict[str, Any] = {\n",
    "    # Data\n",
    "    \"data_root\": data_root,\n",
    "    \"batch\": 32,  # Reduced from 128 for MPS memory\n",
    "    \"data_loader_kwargs\": {\n",
    "        \"num_workers\": 0,\n",
    "        \"pin_memory\": False,\n",
    "        \"persistent_workers\": False,\n",
    "        # PPGDaliaConfig parameters\n",
    "        \"subjects_train\": TRAIN_SUBJECTS,\n",
    "        \"subject_val\": VAL_SUBJECT,\n",
    "        \"subject_test\": TEST_SUBJECT,\n",
    "        \"fs_in\": 64.0,        # Input sampling rate (Hz)\n",
    "        \"fs\": 100.0,          # Target sampling rate (Hz)\n",
    "        \"win_sec\": 8,         # Window length (seconds)\n",
    "        \"stride_sec\": 2,      # Stride (seconds)\n",
    "        \"do_bandpass\": True,\n",
    "        \"low_hz\": 0.5,\n",
    "        \"high_hz\": 8.0,\n",
    "    },\n",
    "\n",
    "    # Training\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 5e-4,\n",
    "    \"wd\": 1e-4,\n",
    "    \"amp\": False,  # Disable AMP for MPS stability\n",
    "    \"save_dir\": \"./runs/ppg_lmu_task\",\n",
    "    \"warmup_epochs\": 5,\n",
    "    \"patience\": 10,\n",
    "    \"min_delta\": 0.01,  # MAE improvement threshold\n",
    "\n",
    "    # Model (reduced for MPS memory)\n",
    "    \"d_model\": 128,      # Reduced from 256\n",
    "    \"depth\": 4,          # Reduced from 6\n",
    "    \"dropout\": 0.2,\n",
    "    \"mlp_ratio\": 2.0,\n",
    "    \"droppath_final\": 0.1,\n",
    "    \"layerscale_init\": 1e-2,\n",
    "    \"residual_gain\": 1.0,\n",
    "    \"pool\": \"mean\",\n",
    "\n",
    "    # LMU-specific\n",
    "    \"memory_size\": 128,  # Reduced from 256\n",
    "}\n",
    "\n",
    "args[\"block_cfg_ctor\"] = create_block_cfg_ctor(\n",
    "    dropout=args[\"dropout\"],\n",
    "    mlp_ratio=args[\"mlp_ratio\"],\n",
    "    droppath_final=args[\"droppath_final\"],\n",
    "    layerscale_init=args[\"layerscale_init\"],\n",
    "    residual_gain=args[\"residual_gain\"],\n",
    "    pool=args[\"pool\"],\n",
    "    memory_size=args[\"memory_size\"],\n",
    ")\n",
    "\n",
    "# Device selection\n",
    "import os\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    args[\"device\"] = torch.device(\"mps\")\n",
    "    # Set MPS memory management BEFORE any operations\n",
    "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    torch.mps.set_per_process_memory_fraction(0.7)\n",
    "    print(\"üöÄ Using MPS (Apple Silicon) with reduced memory\")\n",
    "elif torch.cuda.is_available():\n",
    "    args[\"device\"] = torch.device(\"cuda\")\n",
    "    print(\"üöÄ Using CUDA\")\n",
    "else:\n",
    "    args[\"device\"] = torch.device(\"cpu\")\n",
    "    args[\"amp\"] = False\n",
    "    print(\"‚ö†Ô∏è  Using CPU\")\n",
    "\n",
    "print(f\"\\nüìÇ Data root: {args['data_root']}\")\n",
    "print(f\"üéØ Train subjects: {TRAIN_SUBJECTS}\")\n",
    "print(f\"üéØ Val subject: {VAL_SUBJECT}\")\n",
    "print(f\"üéØ Test subject: {TEST_SUBJECT}\")\n"
   ],
   "id": "ee289b9a6093f985",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using MPS (Apple Silicon) with reduced memory\n",
      "\n",
      "üìÇ Data root: /Users/glbrlb/PycharmProjects/Msc/LMU_S4/src/datasets/ppg/data\n",
      "üéØ Train subjects: ('S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8')\n",
      "üéØ Val subject: S9\n",
      "üéØ Test subject: S10\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "b9994a4208bddb2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:34:51.501142Z",
     "start_time": "2025-11-18T21:18:34.840749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.train_utils.trainer import Trainer\n",
    "from src.models.v2.build_model import build_model\n",
    "\n",
    "# Define the task\n",
    "task = PPGTask()\n",
    "\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(args=args, task=task, model_builder=build_model)\n",
    "\n",
    "# Train\n",
    "best_metric, best_path = trainer.fit()\n",
    "\n",
    "history = trainer.history\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Best validation {trainer.early_key}: {best_metric:.4f}\")\n",
    "print(f\"üíæ Best model saved to: {best_path}\")\n"
   ],
   "id": "c7cd9514f535b820",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glbrlb/PycharmProjects/Msc/LMU_S4/.venv/lib/python3.9/site-packages/scipy/linalg/_matfuncs.py:326: RuntimeWarning: divide by zero encountered in matmul\n",
      "  m, s = pick_pade_structure(Am)\n",
      "/Users/glbrlb/PycharmProjects/Msc/LMU_S4/.venv/lib/python3.9/site-packages/scipy/linalg/_matfuncs.py:326: RuntimeWarning: overflow encountered in matmul\n",
      "  m, s = pick_pade_structure(Am)\n",
      "/Users/glbrlb/PycharmProjects/Msc/LMU_S4/.venv/lib/python3.9/site-packages/scipy/linalg/_matfuncs.py:326: RuntimeWarning: invalid value encountered in matmul\n",
      "  m, s = pick_pade_structure(Am)\n",
      "/Users/glbrlb/PycharmProjects/Msc/LMU_S4/.venv/lib/python3.9/site-packages/scipy/linalg/_matfuncs.py:358: RuntimeWarning: divide by zero encountered in matmul\n",
      "  eAw = eAw @ eAw\n",
      "/Users/glbrlb/PycharmProjects/Msc/LMU_S4/.venv/lib/python3.9/site-packages/scipy/linalg/_matfuncs.py:358: RuntimeWarning: overflow encountered in matmul\n",
      "  eAw = eAw @ eAw\n",
      "/Users/glbrlb/PycharmProjects/Msc/LMU_S4/.venv/lib/python3.9/site-packages/scipy/linalg/_matfuncs.py:358: RuntimeWarning: invalid value encountered in matmul\n",
      "  eAw = eAw @ eAw\n",
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ saved best model to ./runs/ppg_lmu_task/best.pt\n",
      "‚úÖ new best mse 7965.1578\n",
      "Epoch 000/100 | train 8912.6477/8912.6477 | val 7965.1578/7965.1578 | t 244.4s/10.5s | lr 5.00e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ saved best model to ./runs/ppg_lmu_task/best.pt\n",
      "‚úÖ new best mse 4664.3891\n",
      "Epoch 001/100 | train 6893.1810/6893.1810 | val 4664.3891/4664.3891 | t 267.9s/10.4s | lr 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ saved best model to ./runs/ppg_lmu_task/best.pt\n",
      "‚úÖ new best mse 593.2457\n",
      "Epoch 002/100 | train 2916.3936/2916.3936 | val 593.2457/593.2457 | t 264.5s/10.1s | lr 2.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ saved best model to ./runs/ppg_lmu_task/best.pt\n",
      "‚úÖ new best mse 419.0924\n",
      "Epoch 003/100 | train 601.0572/601.0572 | val 419.0924/419.0924 | t 259.3s/10.0s | lr 3.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glbrlb/PycharmProjects/Msc/LMU_S4/.venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ saved best model to ./runs/ppg_lmu_task/best.pt\n",
      "‚úÖ new best mse 414.0154\n",
      "Epoch 004/100 | train 300.0032/300.0032 | val 414.0154/414.0154 | t 257.8s/10.1s | lr 4.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ saved best model to ./runs/ppg_lmu_task/best.pt\n",
      "‚úÖ new best mse 345.5797\n",
      "Epoch 005/100 | train 203.5332/203.5332 | val 345.5797/345.5797 | t 256.9s/10.1s | lr 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ saved best model to ./runs/ppg_lmu_task/best.pt\n",
      "‚úÖ new best mse 230.4436\n",
      "Epoch 006/100 | train 170.5696/170.5696 | val 230.4436/230.4436 | t 257.5s/10.1s | lr 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ saved best model to ./runs/ppg_lmu_task/best.pt\n",
      "‚úÖ new best mse 227.4730\n",
      "Epoch 007/100 | train 146.6591/146.6591 | val 227.4730/227.4730 | t 256.0s/10.0s | lr 4.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008/100 | train 129.3242/129.3242 | val 265.1766/265.1766 | t 257.5s/10.1s | lr 4.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009/100 | train 114.6819/114.6819 | val 236.4995/236.4995 | t 256.0s/10.1s | lr 4.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010/100 | train 102.5112/102.5112 | val 319.3702/319.3702 | t 255.6s/9.9s | lr 4.97e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011/100 | train 90.9796/90.9796 | val 283.3677/283.3677 | t 263.3s/9.9s | lr 4.95e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012/100 | train 82.6397/82.6397 | val 261.4781/261.4781 | t 256.6s/10.2s | lr 4.93e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013/100 | train 76.8817/76.8817 | val 290.6767/290.6767 | t 255.1s/10.0s | lr 4.91e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014/100 | train 69.2895/69.2895 | val 360.9883/360.9883 | t 307.6s/20.6s | lr 4.89e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015/100 | train 62.9593/62.9593 | val 258.3350/258.3350 | t 535.5s/20.4s | lr 4.86e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016/100 | train 58.9457/58.9457 | val 446.3455/446.3455 | t 526.7s/20.8s | lr 4.84e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017/100 | train 54.0642/54.0642 | val 318.8208/318.8208 | t 501.9s/16.2s | lr 4.81e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚èπ Early stopping (patience=10, best=227.4730).\n",
      "üìä Training history saved to ./runs/ppg_lmu_task/history.json\n",
      "\n",
      "‚úÖ Training complete! Best validation mse: 227.4730\n",
      "üíæ Best model saved to: ./runs/ppg_lmu_task/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot History",
   "id": "257d4d4fa44776f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\", linewidth=2)\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"MSE Loss\", fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.title(\"Training & Validation Loss\", fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"train_mae\"], label=\"train_mae\", linewidth=2)\n",
    "plt.plot(history[\"val_mae\"], label=\"val_mae\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"MAE (bpm)\", fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.title(\"Mean Absolute Error\", fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "95e0feeb46ba207c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test Evaluation",
   "id": "4a3ab33ff96ab7dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from torch.amp import autocast as amp_autocast\n",
    "\n",
    "def evaluate_best_model(args: dict, task: TaskProtocol, model_builder: callable, best_model_path: str):\n",
    "    \"\"\"Load best checkpoint and evaluate on test set.\"\"\"\n",
    "    print(\"üìä Evaluating best model on the test set...\")\n",
    "\n",
    "    # 1. Test loader\n",
    "    _, _, test_loader = task.make_loaders(\n",
    "        data_root=args[\"data_root\"],\n",
    "        batch_size=args[\"batch\"],\n",
    "        **args[\"data_loader_kwargs\"]\n",
    "    )\n",
    "\n",
    "    # 2. Load checkpoint and rebuild model\n",
    "    device = args.get(\"device\", torch.device(\"cpu\"))\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "\n",
    "    flat_args = dict(args)\n",
    "    flat_args.update(args.get(\"data_loader_kwargs\", {}))\n",
    "    d_in = task.infer_input_dim(flat_args)\n",
    "    n_classes = task.infer_num_classes(flat_args)\n",
    "    theta = task.infer_theta(flat_args)\n",
    "\n",
    "    block_cfg = args[\"block_cfg_ctor\"](theta)\n",
    "    model = model_builder(\n",
    "        d_in=d_in,\n",
    "        n_classes=n_classes,\n",
    "        d_model=args[\"d_model\"],\n",
    "        depth=args[\"depth\"],\n",
    "        block_cfg=block_cfg\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"‚úÖ Loaded checkpoint from epoch {checkpoint.get('epoch', 'N/A')}\")\n",
    "    val_metrics = checkpoint.get('val', {})\n",
    "    print(f\"üìà Val MSE: {val_metrics.get('mse', 'N/A'):.4f}, Val MAE: {val_metrics.get('mae', 'N/A'):.4f}\")\n",
    "\n",
    "    # 3. Evaluation loop\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            # Disable AMP for evaluation\n",
    "            with amp_autocast(device_type=device.type, enabled=False):\n",
    "                out = model(x)\n",
    "            all_preds.append(out.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "\n",
    "    # 4. Metrics\n",
    "    all_preds = np.concatenate(all_preds).flatten()\n",
    "    all_targets = np.concatenate(all_targets).flatten()\n",
    "\n",
    "    test_mse = np.mean((all_preds - all_targets) ** 2)\n",
    "    test_mae = np.mean(np.abs(all_preds - all_targets))\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TEST SET RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"MSE:  {test_mse:.4f}\")\n",
    "    print(f\"MAE:  {test_mae:.4f} bpm\")\n",
    "    print(f\"RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "    return all_preds, all_targets\n",
    "\n",
    "# Run evaluation\n",
    "preds, targets = evaluate_best_model(\n",
    "    args=args,\n",
    "    task=task,\n",
    "    model_builder=build_model,\n",
    "    best_model_path=best_path\n",
    ")\n"
   ],
   "id": "995781b0d9af7128",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
